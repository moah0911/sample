

# **S.A.G.E: Spiritual AI-powered Gita and Enlightenment**

**S.A.G.E.** (Spiritual AI-powered Gita and Enlightenment) is an advanced AI-powered pipeline designed to retrieve relevant verses from the **Bhagavad Gita** and **Patanjali Yoga Sutras (PYS)** based on user queries. By leveraging state-of-the-art **Information Retrieval (IR)** techniques and an open-source **Large Language Model (LLM)** like **Meta-LLaMA-3.1-8B-Instruct**, this project provides contextually accurate summaries for spiritual queries. The system ensures **semantic search**, efficient **quantized models**, and **AI ethics** in handling sacred texts.

## **Core Features**
1. **Context-Aware Verse Retrieval**: Uses semantic search and ranking algorithms to identify relevant verses.
2. **LLM-Driven Summarization**: Generates concise and contextually relevant answers using an open-source LLM.
3. **Robust Query Handling**: Flags irrelevant or inappropriate queries using AI ethics considerations.
4. **Interoperable JSON Outputs**: Delivers results in structured JSON format for integration with APIs or other systems.
5. **Efficient Quantized Models**: Uses quantized models to reduce resource usage while maintaining high accuracy.
6. **Sentence Transformers**: Enhances retrieval by computing sentence embeddings for improved semantic search.

## **Project Structure**

```
project-root/
├── data/
│   ├── Bhagwad_Gita_Verses_Concepts.csv
│   ├── Bhagwad_Gita_Verses_English.csv
│   ├── Bhagwad_Gita_Verses_English_Questions.csv
│   ├── Gita_Word_Meanings_English.csv
│   ├── Gita_Word_Meanings_Hindi.csv
│   ├── Patanjali_Yoga_Sutras_Verses_English.csv
│   ├── Patanjali_Yoga_Sutras_Verses_English_Questions.csv
├── models/
│   └── Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
├── src/
│   ├── config.py       # Configuration management
│   ├── llm.py          # LLM integration for summarization
│   ├── retriever.py    # Semantic search for verse extraction
│   ├── utils.py        # Utility functions for data preprocessing
│   
├── main.py             # Entry point for executing the pipeline
├── requirements.txt    # Dependencies file
└── LICENSE             # License file
```

## **Getting Started**

### Step 1: Environment Setup

1. **Clone the repository**:
   ```bash
   git clone <repository_url>
   cd <repository_name>
   ```

2. **Install required dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Download and place the model**:
   Ensure the LLaMA model file (`Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf`) is placed in the `models/` directory.

### Step 2: Running the Pipeline

To start processing user queries, execute the following:

```bash
python main.py
```

### Example Query Input

Provide a query in JSON format:

```json
{
  "query": "What guidance does the Bhagavad Gita provide about karma?"
}
```

### Example Pipeline Output

```json
{
  "query": "What guidance does the Bhagavad Gita provide about karma?",
  "retrieved_verses": [
    {
      "verse": "Bhagavad Gita 2.47",
      "text": "You have a right to perform your prescribed duties, but you are not entitled to the fruits of your actions..."
    }
  ],
  "summary": "The Bhagavad Gita emphasizes performing duties with dedication while detaching oneself from the results."
}
```

## **Model and Retrieval Techniques**

### Quantized Models

- **Essence**: The project uses **quantized models**, reducing the memory and computation requirements without compromising performance. This optimization ensures the model is efficient, scalable, and capable of running on devices with lower hardware specifications, replacing traditional large models that require extensive resources.
  
### Sentence Transformers

- **Improved Semantic Search**: By integrating the **`sentence-transformers`** Python module, the retrieval process is enhanced. Sentences are embedded into high-dimensional vector space for better semantic understanding, enabling the system to retrieve the most contextually relevant verses.

## **AI Ethics**

Ethical considerations are critical in this project, particularly because it involves sacred and culturally significant texts like the **Bhagavad Gita** and **Patanjali Yoga Sutras**. The AI system should be designed and utilized in a way that respects these texts' spiritual, religious, and philosophical meanings. 

### **Key Ethical Principles**:

1. **Accuracy and Respect**: The AI system is programmed to ensure that the summaries generated from the verses remain **theologically accurate** and **contextually respectful**. This includes preventing misinterpretations or manipulations of the verses' deeper meanings. The LLM used in the system has been carefully integrated to provide concise answers that stay true to the original context of the texts.

2. **Content Moderation**: Given the sensitive nature of religious texts, a robust content moderation mechanism is essential to ensure that **irrelevant, harmful, or offensive queries** are flagged and filtered out. In addition, the system is designed to reject any queries that could mislead users or promote unethical viewpoints.

3. **Transparency**: The AI's decision-making process must be transparent. Users should understand how the answers are generated, ensuring that responses are **derived from the text** itself, based on contextually relevant verses.

4. **Cultural Sensitivity**: Since the project involves **sacred Indian texts**, special care is taken to respect the spiritual, cultural, and religious significance of the content. The project values inclusivity, ensuring that interpretations are grounded in authentic teachings and philosophies, while also being sensitive to the diverse beliefs of users.

5. **Ethical Collaboration**: The project may collaborate with **spiritual scholars** or **religious experts** to continuously evaluate and improve the system’s output, ensuring it aligns with traditional interpretations and ethical standards.

By embedding these ethical principles into the design and functionality, **S.A.G.E.** ensures that it serves as a respectful, accurate, and responsible tool for exploring spiritual and philosophical teachings.

## **Query and Output Validation**
The project incorporates various validation mechanisms, ensuring that queries are respectful and the outputs generated are both accurate and appropriate. These checks prevent the system from producing harmful, misleading, or culturally insensitive responses.

## **Evaluation and Performance**

### Metrics

1. **Retrieval Accuracy**: Precision of retrieved verses based on a set of curated queries.
2. **Summarization Quality**: Relevance and coherence of the generated answers.
3. **Efficiency**: Latency and resource usage per query.

### Benchmarking

- **Domain Expert Validation**: Outputs are validated for theological and contextual correctness.
- **Scalability**: Optimized to handle large datasets with minimal latency using quantized models and semantic search.

## **Future Enhancements**

1. **Multilingual Support**: Expand the retrieval and summarization capabilities to include Hindi and Sanskrit.
2. **Fine-Tuning**: Improve the LLM's performance by fine-tuning it on domain-specific datasets for more accurate summaries.
3. **Scalability**: Implement more advanced retrieval techniques to handle large-scale datasets with minimal latency.
4. **Interactive UI**: Develop a web-based or chatbot interface for better user interaction.

## **Contribution Guidelines**

Contributions are welcome! To contribute:

1. Fork the repository.
2. Create a new feature branch.
3. Commit your changes.
4. Submit a pull request with a detailed description.

## **License**

This project is licensed under the **MIT License**. Please refer to the `LICENSE` file for more details.

---

### Contact

For further inquiries or support, contact us at: [your_email@example.com].

--- 

